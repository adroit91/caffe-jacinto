name: "ConvNet-10(8)"

input: "data"
input_shape {
  dim: 1
  dim: 3
  dim: 512
  dim: 1024
}

layer {
  name: "data_bias"
  bottom: "data"
  top: "data_bias"
  type: "Bias"
  param { #scale
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}

layer {
  name: "conv1a"
  bottom: "data_bias"
  top: "conv1a"
  type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 }
  param { lr_mult: 2 decay_mult: 0 }  
  convolution_param {
    num_output: 32
    kernel_size: 5
    pad: 2
    stride: 2
    bias_term: true
    weight_filler { type: "msra" std: 0.010 }
    bias_filler { type: "constant" value: 0 }    
    dilation: 1    
    group: 1
  }
}
layer {
  name: "bn_conv1a"
  bottom: "conv1a"
  top: "conv1a/bn"
  type: "BatchNorm"
  param { #scale
    lr_mult: 1
    decay_mult: 1
  }
  param { #shift/bias
   lr_mult: 1
    decay_mult: 1
  } 
  param { #global mean
    lr_mult: 0
    decay_mult: 0
  }
  param { #global var
   lr_mult: 0
    decay_mult: 0
  }     
  batch_norm_param {
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }  
    moving_average_fraction: 0.99
    eps: 0.0001    
  }
}
layer {
  name: "conv1a_relu"
  bottom: "conv1a/bn"
  top: "conv1a/bn"
  type: "ReLU"
}
layer {
  name: "conv1b"
  bottom: "conv1a/bn"
  top: "conv1b"
  type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 }
  param { lr_mult: 2 decay_mult: 0 }  
  convolution_param {
    num_output: 32
    kernel_size: 3
    pad: 1
    stride: 1
    bias_term: true
    weight_filler { type: "msra" std: 0.010 }
    bias_filler { type: "constant" value: 0 }    
    dilation: 1    
    group: 4 #1
  }
}
layer {
  name: "bn_conv1b"
  bottom: "conv1b"
  top: "conv1b/bn"
  type: "BatchNorm"
  param { #scale
    lr_mult: 1
    decay_mult: 1
  }
  param { #shift/bias
   lr_mult: 1
    decay_mult: 1
  } 
  param { #global mean
    lr_mult: 0
    decay_mult: 0
  }
  param { #global var
   lr_mult: 0
    decay_mult: 0
  }     
  batch_norm_param {
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }  
    moving_average_fraction: 0.99
    eps: 0.0001    
  }
}
layer {
  name: "conv1b_relu"
  bottom: "conv1b/bn"
  top: "conv1b/bn"
  type: "ReLU"
}
layer {
  name: "res2a_branch2a"
  bottom: "conv1b/bn"
  top: "res2a_branch2a"
  type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 }
  param { lr_mult: 2 decay_mult: 0 }  
  convolution_param {
    num_output: 64
    kernel_size: 3
    pad: 1
    stride: 2
    bias_term: true
    weight_filler { type: "msra" std: 0.010 }
    bias_filler { type: "constant" value: 0 }    
    dilation: 1    
    group: 1 #4
  }
}
layer {
  name: "bn2a_branch2a"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a/bn"
  type: "BatchNorm"
  param { #scale
    lr_mult: 1
    decay_mult: 1
  }
  param { #shift/bias
   lr_mult: 1
    decay_mult: 1
  } 
  param { #global mean
    lr_mult: 0
    decay_mult: 0
  }
  param { #global var
   lr_mult: 0
    decay_mult: 0
  }     
  batch_norm_param {
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }  
    moving_average_fraction: 0.99
    eps: 0.0001    
  }
}
layer {
  name: "res2a_branch2a_relu"
  bottom: "res2a_branch2a/bn"
  top: "res2a_branch2a/bn"
  type: "ReLU"
}
layer {
  name: "res2a_branch2b"
  bottom: "res2a_branch2a/bn"
  top: "res2a_branch2b"
  type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 }
  param { lr_mult: 2 decay_mult: 0 }  
  convolution_param {
    num_output: 64
    kernel_size: 3
    pad: 1
    stride: 1
    bias_term: true
    weight_filler { type: "msra" std: 0.010 }
    bias_filler { type: "constant" value: 0 }    
    dilation: 1    
    group: 4
  }
}
layer {
  name: "bn2a_branch2b"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b/bn"
  type: "BatchNorm"
  param { #scale
    lr_mult: 1
    decay_mult: 1
  }
  param { #shift/bias
   lr_mult: 1
    decay_mult: 1
  } 
  param { #global mean
    lr_mult: 0
    decay_mult: 0
  }
  param { #global var
   lr_mult: 0
    decay_mult: 0
  }     
  batch_norm_param {
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }  
    moving_average_fraction: 0.99
    eps: 0.0001    
  }
}
layer {
  name: "res2a_relu"
  bottom: "res2a_branch2b/bn"
  top: "res2a_branch2b/bn"
  type: "ReLU"
}
layer {
  name: "res3a_branch2a"
  bottom: "res2a_branch2b/bn"
  top: "res3a_branch2a"
  type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 }
  param { lr_mult: 2 decay_mult: 0 }  
  convolution_param {
    num_output: 128
    kernel_size: 3
    pad: 1
    stride: 2
    bias_term: true
    weight_filler { type: "msra" std: 0.010 }
    bias_filler { type: "constant" value: 0 }    
    dilation: 1    
    group: 1 #4
  }
}
layer {
  name: "bn3a_branch2a"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a/bn"
  type: "BatchNorm"
  param { #scale
    lr_mult: 1
    decay_mult: 1
  }
  param { #shift/bias
   lr_mult: 1
    decay_mult: 1
  } 
  param { #global mean
    lr_mult: 0
    decay_mult: 0
  }
  param { #global var
   lr_mult: 0
    decay_mult: 0
  }     
  batch_norm_param {
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }  
    moving_average_fraction: 0.99
    eps: 0.0001    
  }
}
layer {
  name: "res3a_branch2a_relu"
  bottom: "res3a_branch2a/bn"
  top: "res3a_branch2a/bn"
  type: "ReLU"
}
layer {
  name: "res3a_branch2b"
  bottom: "res3a_branch2a/bn"
  top: "res3a_branch2b"
  type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 }
  param { lr_mult: 2 decay_mult: 0 }  
  convolution_param {
    num_output: 128
    kernel_size: 3
    pad: 1
    stride: 1
    bias_term: true
    weight_filler { type: "msra" std: 0.010 }
    bias_filler { type: "constant" value: 0 }    
    dilation: 1    
    group: 4
  }
}
layer {
  name: "bn3a_branch2b"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b/bn"
  type: "BatchNorm"
  param { #scale
    lr_mult: 1
    decay_mult: 1
  }
  param { #shift/bias
   lr_mult: 1
    decay_mult: 1
  } 
  param { #global mean
    lr_mult: 0
    decay_mult: 0
  }
  param { #global var
   lr_mult: 0
    decay_mult: 0
  }     
  batch_norm_param {
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }  
    moving_average_fraction: 0.99
    eps: 0.0001    
  }
}
layer {
  name: "res3a_relu"
  bottom: "res3a_branch2b/bn"
  top: "res3a_branch2b/bn"
  type: "ReLU"
}
layer {
  name: "res4a_branch2a"
  bottom: "res3a_branch2b/bn"
  top: "res4a_branch2a"
  type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 }
  param { lr_mult: 2 decay_mult: 0 }  
  convolution_param {
    num_output: 256
    kernel_size: 3
    pad: 1
    stride: 2
    bias_term: true
    weight_filler { type: "msra" std: 0.010 }
    bias_filler { type: "constant" value: 0 }    
    dilation: 1    
    group: 1 #4
  }
}
layer {
  name: "bn4a_branch2a"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a/bn"
  type: "BatchNorm"
  param { #scale
    lr_mult: 1
    decay_mult: 1
  }
  param { #shift/bias
   lr_mult: 1
    decay_mult: 1
  } 
  param { #global mean
    lr_mult: 0
    decay_mult: 0
  }
  param { #global var
   lr_mult: 0
    decay_mult: 0
  }     
  batch_norm_param {
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }  
    moving_average_fraction: 0.99
    eps: 0.0001    
  }
}
layer {
  name: "res4a_branch2a_relu"
  bottom: "res4a_branch2a/bn"
  top: "res4a_branch2a/bn"
  type: "ReLU"
}
layer {
  name: "res4a_branch2b"
  bottom: "res4a_branch2a/bn"
  top: "res4a_branch2b"
  type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 }
  param { lr_mult: 2 decay_mult: 0 }  
  convolution_param {
    num_output: 256
    kernel_size: 3
    pad: 1
    stride: 1
    bias_term: true
    weight_filler { type: "msra" std: 0.010 }
    bias_filler { type: "constant" value: 0 }    
    dilation: 1    
    group: 4
  }
}
layer {
  name: "bn4a_branch2b"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b/bn"
  type: "BatchNorm"
  param { #scale
    lr_mult: 1
    decay_mult: 1
  }
  param { #shift/bias
   lr_mult: 1
    decay_mult: 1
  } 
  param { #global mean
    lr_mult: 0
    decay_mult: 0
  }
  param { #global var
   lr_mult: 0
    decay_mult: 0
  }     
  batch_norm_param {
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }  
    moving_average_fraction: 0.99
    eps: 0.0001    
  }
}
layer {
  name: "res4a_relu"
  bottom: "res4a_branch2b/bn"
  top: "res4a_branch2b/bn"
  type: "ReLU"
}
layer {
  name: "res5a_branch2a"
  bottom: "res4a_branch2b/bn"
  top: "res5a_branch2a"
  type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 }
  param { lr_mult: 2 decay_mult: 0 }  
  convolution_param {
    num_output: 512
    kernel_size: 3
    pad: 1
    stride: 1
    bias_term: true
    weight_filler { type: "msra" std: 0.010 }
    bias_filler { type: "constant" value: 0 }    
    dilation: 1    
    group: 1 #4
  }
}
layer {
  name: "bn5a_branch2a"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a/bn"
  type: "BatchNorm"
  param { #scale
    lr_mult: 1
    decay_mult: 1
  }
  param { #shift/bias
   lr_mult: 1
    decay_mult: 1
  } 
  param { #global mean
    lr_mult: 0
    decay_mult: 0
  }
  param { #global var
   lr_mult: 0
    decay_mult: 0
  }     
  batch_norm_param {
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }  
    moving_average_fraction: 0.99
    eps: 0.0001    
  }
}
layer {
  name: "res5a_branch2a_relu"
  bottom: "res5a_branch2a/bn"
  top: "res5a_branch2a/bn"
  type: "ReLU"
}
layer {
  name: "res5a_branch2b"
  bottom: "res5a_branch2a/bn"
  top: "res5a_branch2b"
  type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 }
  param { lr_mult: 2 decay_mult: 0 }  
  convolution_param {
    num_output: 512
    kernel_size: 3
    pad: 2
    stride: 1
    bias_term: true
    weight_filler { type: "msra" std: 0.010 }
    bias_filler { type: "constant" value: 0 }    
    dilation: 2    
    group: 4
  }
}
layer {
  name: "bn5a_branch2b"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b/bn"
  type: "BatchNorm"
  param { #scale
    lr_mult: 1
    decay_mult: 1
  }
  param { #shift/bias
   lr_mult: 1
    decay_mult: 1
  } 
  param { #global mean
    lr_mult: 0
    decay_mult: 0
  }
  param { #global var
   lr_mult: 0
    decay_mult: 0
  }     
  batch_norm_param {
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }  
    moving_average_fraction: 0.99
    eps: 0.0001    
  }
}
layer {
  name: "res5a_relu"
  bottom: "res5a_branch2b/bn"
  top: "res5a_branch2b/bn"
  type: "ReLU"
}

#------------------------------------------------------
#Output layers
#------------------------------------------------------
layer {
  name: "out5a"
  type: "Convolution"
  bottom: "res5a_branch2b/bn"
  top: "out5a"
  param { lr_mult: 1 decay_mult: 1 }
  param { lr_mult: 2 decay_mult: 0 }    
  convolution_param {
    num_output: 64
    kernel_size: 3
    pad: 4
    bias_term: true    
    weight_filler { type: "msra" std: 0.010 }
    bias_filler { type: "constant" value: 0 } 
    dilation: 4
    group: 2 
  }  
}
layer {
	bottom: "out5a"
	top: "out5a/bn"
	name: "bn_out5a"
	type: "BatchNorm"
    param { #scale
      lr_mult: 1
      decay_mult: 1
    }
    param { #shift/bias
     lr_mult: 1
      decay_mult: 1
    } 
    param { #global mean
      lr_mult: 0
      decay_mult: 0
    }
    param { #global var
     lr_mult: 0
      decay_mult: 0
    }     
    batch_norm_param {
      scale_filler {
        type: "constant"
        value: 1
      }
      bias_filler {
        type: "constant"
        value: 0
      }
      moving_average_fraction: 0.99
      eps: 0.0001        
    }
}
layer {
  name: "out5a_relu"
  type: "ReLU"
  bottom: "out5a/bn"
  top: "out5a/bn"  
}
layer {
  name: "out5_up2"
  type: "Deconvolution"
  bottom: "out5a/bn"
  top: "out5_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 64
    stride: 2
    weight_filler {
      type: "bilinear"
    }    
  }
}
layer {
  name: "out3a"
  type: "Convolution"
  bottom: "res3a_branch2b/bn"
  top: "out3a"
  param { lr_mult: 1 decay_mult: 1 }
  param { lr_mult: 2 decay_mult: 0 }    
  convolution_param {
    num_output: 64
    kernel_size: 3
    pad: 1
    bias_term: true    
    weight_filler { type: "msra" std: 0.010 }
    bias_filler { type: "constant" value: 0 } 
    dilation: 1
    group: 2
  }  
}
layer {
	bottom: "out3a"
	top: "out3a/bn"
	name: "bn_out3a"
	type: "BatchNorm"
    param { #scale
      lr_mult: 1
      decay_mult: 1
    }
    param { #shift/bias
     lr_mult: 1
      decay_mult: 1
    } 
    param { #global mean
      lr_mult: 0
      decay_mult: 0
    }
    param { #global var
     lr_mult: 0
      decay_mult: 0
    }     
    batch_norm_param {
      scale_filler {
        type: "constant"
        value: 1
      }
      bias_filler {
        type: "constant"
        value: 0
      }
      moving_average_fraction: 0.99
      eps: 0.0001        
    }
}
layer {
  name: "out3a_relu"
  type: "ReLU"
  bottom: "out3a/bn"
  top: "out3a/bn"
}
layer {
	name: "out3_out5_combined"
	bottom: "out5_up2"
	bottom: "out3a/bn"
	top: "out3_out5_combined"
	type: "Eltwise"
}

#----------------------------------------------------------------------------
#Additional ctx layers
#----------------------------------------------------------------------------
layer {
  name: "ctx_conv1"
  type: "Convolution"
  bottom: "out3_out5_combined"
  top: "ctx_conv1"
  param { lr_mult: 1 decay_mult: 1.0 }
  param { lr_mult: 2 decay_mult: 0 }    
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    bias_term: true       
    weight_filler { type: "msra" std: 0.010 }
    bias_filler { type: "constant" value: 0 }  
    dilation: 1
    group: 1     
  }  
}

layer {
	bottom: "ctx_conv1"
	top: "ctx_conv1/bn"
	name: "bn_ctx_conv1"
	type: "BatchNorm"
    param { #scale
      lr_mult: 1
      decay_mult: 1
    }
    param { #shift/bias
     lr_mult: 1
      decay_mult: 1
    } 
    param { #global mean
      lr_mult: 0
      decay_mult: 0
    }
    param { #global var
     lr_mult: 0
      decay_mult: 0
    }     
    batch_norm_param {
      scale_filler {
        type: "constant"
        value: 1
      }
      bias_filler {
        type: "constant"
        value: 0
      }
      moving_average_fraction: 0.99
      eps: 0.0001       
    }
}

layer {
  name: "ctx_relu1"
  type: "ReLU"
  bottom: "ctx_conv1/bn"
  top: "ctx_conv1/bn"
}

layer {
  name: "ctx_conv2"
  type: "Convolution"
  bottom: "ctx_conv1/bn"
  top: "ctx_conv2"
  param { lr_mult: 1 decay_mult: 1.0 }
  param { lr_mult: 2 decay_mult: 0 }    
  convolution_param {
    num_output: 64
    pad: 4
    kernel_size: 3
    bias_term: true       
    weight_filler { type: "msra" std: 0.010 }
    bias_filler { type: "constant" value: 0 } 
    dilation: 4
    group: 1     
  } 
}

layer {
	bottom: "ctx_conv2"
	top: "ctx_conv2/bn"
	name: "bn_ctx_conv2"
	type: "BatchNorm"
    param { #scale
      lr_mult: 1
      decay_mult: 1
    }
    param { #shift/bias
     lr_mult: 1
      decay_mult: 1
    } 
    param { #global mean
      lr_mult: 0
      decay_mult: 0
    }
    param { #global var
     lr_mult: 0
      decay_mult: 0
    }     
    batch_norm_param {
      scale_filler {
        type: "constant"
        value: 1
      }
      moving_average_fraction: 0.99
      eps: 0.0001       
    }
}

layer {
  name: "ctx_relu2"
  type: "ReLU"
  bottom: "ctx_conv2/bn"
  top: "ctx_conv2/bn" 
}

layer {
  name: "ctx_conv3"
  type: "Convolution"
  bottom: "ctx_conv2/bn"
  top: "ctx_conv3"
  param { lr_mult: 1 decay_mult: 1.0 }
  param { lr_mult: 2 decay_mult: 0 }    
  convolution_param {
    num_output: 64
    pad: 4
    kernel_size: 3
    bias_term: true       
    weight_filler { type: "msra" std: 0.010 }
    bias_filler { type: "constant" value: 0 } 
    dilation: 4
    group: 1      
  }
}

layer {
	bottom: "ctx_conv3"
	top: "ctx_conv3/bn"
	name: "bn_ctx_conv3"
	type: "BatchNorm"
    param { #scale
      lr_mult: 1
      decay_mult: 1
    }
    param { #shift/bias
     lr_mult: 1
      decay_mult: 1
    } 
    param { #global mean
      lr_mult: 0
      decay_mult: 0
    }
    param { #global var
     lr_mult: 0
      decay_mult: 0
    }     
    batch_norm_param {
      scale_filler {
        type: "constant"
        value: 1
      }
      bias_filler {
        type: "constant"
        value: 0
      }
      moving_average_fraction: 0.99
      eps: 0.0001       
    }
}


layer {
  name: "ctx_relu3"
  type: "ReLU"
  bottom: "ctx_conv3/bn"
  top: "ctx_conv3/bn"
}

layer {
  name: "ctx_conv4"
  type: "Convolution"
  bottom: "ctx_conv3/bn"
  top: "ctx_conv4"
  param { lr_mult: 1 decay_mult: 1.0 }
  param { lr_mult: 2 decay_mult: 0 }    
  convolution_param {
    num_output: 64
    pad: 4
    kernel_size: 3
    bias_term: true       
    weight_filler { type: "msra" std: 0.010 }
    bias_filler { type: "constant" value: 0 } 
    dilation: 4
    group: 1        
  }  
}

layer {
	bottom: "ctx_conv4"
	top: "ctx_conv4/bn"
	name: "bn_ctx_conv4"
	type: "BatchNorm"
    param { #scale
      lr_mult: 1
      decay_mult: 1
    }
    param { #shift/bias
     lr_mult: 1
      decay_mult: 1
    } 
    param { #global mean
      lr_mult: 0
      decay_mult: 0
    }
    param { #global var
     lr_mult: 0
      decay_mult: 0
    }     
    batch_norm_param {
      scale_filler {
        type: "constant"
        value: 1
      }
      bias_filler {
        type: "constant"
        value: 0
      }
      moving_average_fraction: 0.99
      eps: 0.0001       
    }
}

layer {
  name: "ctx_conv4_relu"
  type: "ReLU"
  bottom: "ctx_conv4/bn"
  top: "ctx_conv4/bn"  
}

layer {
  name: "ctx_final"
  type: "Convolution"
  bottom: "ctx_conv4/bn"
  top: "ctx_final"
  param { lr_mult: 1 decay_mult: 1.0 }
  param { lr_mult: 2 decay_mult: 0 }    
  convolution_param {
    num_output: 8
    kernel_size: 3
    pad: 1
    bias_term: true       
    weight_filler { type: "msra" std: 0.010 }
    bias_filler { type: "constant" value: 0 }   
    dilation: 1   
    group: 1      
  }   
}

layer {
  name: "ctx_final_relu"
  type: "ReLU"
  bottom: "ctx_final"
  top: "ctx_final"     
}

#--------------------------------------------
#Final deconvolution layers
#--------------------------------------------
layer {
  name: "out_deconv_final_up2"
  type: "Deconvolution"
  bottom: "ctx_final"
  top: "out_deconv_final_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }    
  }
}
layer {
  name: "out_deconv_final_up4"
  type: "Deconvolution"
  bottom: "out_deconv_final_up2"
  top: "out_deconv_final_up4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }    
  }
}
layer {
  name: "out_deconv_final_up8"
  type: "Deconvolution"
  bottom: "out_deconv_final_up4"
  top: "out_deconv_final_up8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }    
  }
}

layer {
  name: "prob"
  type: "Softmax"
  bottom: "out_deconv_final_up8"
  top: "prob"
}

layer {
  name: "argMaxOut"
  type: "ArgMax"
  bottom: "out_deconv_final_up8"
  top: "argMaxOut"
  argmax_param {
    axis: 1
  }
}

